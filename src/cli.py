"""
CLI Tool - Command-line interface for AI SimTest.
Run simulations directly from the terminal.
"""

from __future__ import annotations

import asyncio
import json
import sys
from pathlib import Path

import click
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table

console = Console()


@click.group()
@click.version_option(version="0.2.0")
def main():
    """AI SimTest - Open-source AI simulation testing platform."""
    pass


# ============================================================
# simtest run
# ============================================================

@main.command()
@click.option("--bot-endpoint", required=True, help="API endpoint of the bot to test")
@click.option("--bot-api-key", default=None, help="API key for the bot")
@click.option("--bot-format", default="openai", help="Request format: openai, anthropic, custom")
@click.option("--mode", default="manual", type=click.Choice(["manual", "partial"]), help="Mode: manual (provide everything) or partial (AI derives config from docs)")
@click.option("--doc-dir", default=None, help="Directory of bot documentation (for partial mode: txt, md, pdf, docx, html, json)")
@click.option("--documentation", default="", help="Inline documentation text for grounding")
@click.option("--doc-file", default=None, help="Path to documentation file (txt, md, json) for grounding judge")
@click.option("--success-criteria", default=None, multiple=True, help="Success criteria (can repeat: --success-criteria 'rule1' --success-criteria 'rule2')")
@click.option("--topics", default=None, help="Comma-separated test topics (e.g., 'refunds,shipping,billing')")
@click.option("--name", default="CLI Simulation Run", help="Name for this simulation run")
@click.option("--personas", default=20, help="Number of personas to generate")
@click.option("--max-turns", default=15, help="Max conversation turns")
@click.option("--min-turns", default=1, help="Min conversation turns before allowing early exit (default: 1)")
@click.option("--parallel", default=10, help="Max parallel conversations")
@click.option("--output", default="./reports", help="Output directory for reports")
@click.option("--export-formats", default="jsonl,csv,summary,html", help="Comma-separated export formats (jsonl,csv,summary,html,dpo)")
@click.option("--config-file", default=None, help="Path to JSON config file (overrides other options)")
@click.option("--preview-personas", is_flag=True, default=False, help="Generate and preview personas before running simulation")
@click.option("--pass-threshold", default=0.7, help="Score threshold for PASS (0.0-1.0, default 0.7)")
@click.option("--warn-threshold", default=0.5, help="Score threshold for WARNING vs FAIL (0.0-1.0, default 0.5)")
@click.option("--save-suite", "auto_save_suite", is_flag=True, default=False, help="Auto-save a regression suite from failures")
@click.option("--auto-approve", is_flag=True, default=False, help="Auto-approve all gates in partial mode (for CI/CD)")
@click.option("--analysis-only", is_flag=True, default=False, help="Run analysis stages only, skip simulation (partial mode)")
def run(
    bot_endpoint: str,
    bot_api_key: str | None,
    bot_format: str,
    mode: str,
    doc_dir: str | None,
    documentation: str,
    doc_file: str | None,
    success_criteria: tuple[str, ...],
    topics: str | None,
    name: str,
    personas: int,
    max_turns: int,
    min_turns: int,
    parallel: int,
    output: str,
    export_formats: str,
    config_file: str | None,
    preview_personas: bool,
    pass_threshold: float,
    warn_threshold: float,
    auto_save_suite: bool,
    auto_approve: bool,
    analysis_only: bool,
):
    """Run a simulation test against your AI chatbot."""
    from src.core.logging import setup_logging
    setup_logging()

    console.print(Panel.fit(
        "[bold blue]AI SimTest[/] - Simulation Testing Platform",
        subtitle="v0.2.0",
    ))

    # ‚îÄ‚îÄ Partial Autonomous Mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if mode == "partial":
        if not doc_dir and not doc_file:
            console.print("[red]Error: Partial mode requires --doc-dir or --doc-file[/]")
            console.print("  Usage: simtest run --mode partial --doc-dir ./docs/ --bot-endpoint http://bot/api")
            sys.exit(1)

        console.print(f"  ü§ñ Mode: [bold cyan]Partial Autonomous[/]")
        if doc_dir:
            console.print(f"  üìÅ Documentation directory: [bold]{doc_dir}[/]")
        if doc_file:
            console.print(f"  üìÑ Documentation file: [bold]{doc_file}[/]")
        if auto_approve:
            console.print(f"  ‚ö° Auto-approve: [bold green]ON[/] (CI/CD mode)")
        if analysis_only:
            console.print(f"  üîç Analysis only: [bold yellow]ON[/] (no simulation)")

        doc_files_list = [doc_file] if doc_file else None
        sim_name = name if name != "CLI Simulation Run" else "Partial Autonomous Simulation"

        asyncio.run(_run_partial_autonomous(
            bot_endpoint=bot_endpoint,
            bot_api_key=bot_api_key,
            bot_format=bot_format,
            doc_dir=doc_dir,
            doc_files=doc_files_list,
            auto_approve=auto_approve,
            analysis_only=analysis_only,
            output_dir=output,
            export_formats=export_formats.split(","),
            simulation_name=sim_name,
            num_personas=personas,
            max_turns=max_turns,
            min_turns=min_turns,
        ))
        return

    # ‚îÄ‚îÄ Manual Mode (existing behavior) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    console.print(f"  ü§ñ Mode: [bold]Manual[/]")

    # Load documentation from file if path provided
    doc_text = documentation
    if doc_file:
        doc_path = Path(doc_file)
        if doc_path.exists():
            doc_text = doc_path.read_text(encoding="utf-8")
            console.print(f"  üìÑ Loaded documentation from [bold]{doc_file}[/] ({len(doc_text)} chars)")
        else:
            console.print(f"  [red]‚ö† Documentation file not found: {doc_file}[/]")
    elif documentation and Path(documentation).exists():
        doc_text = Path(documentation).read_text(encoding="utf-8")

    # Parse success criteria
    criteria_list = list(success_criteria) if success_criteria else []

    # Parse topics
    topics_list = [t.strip() for t in topics.split(",") if t.strip()] if topics else []

    # Load config from file if provided
    if config_file:
        config_path = Path(config_file)
        if not config_path.exists():
            console.print(f"[red]Error: Config file not found: {config_file}[/]")
            sys.exit(1)

        # Strip comments (lines starting with # or //)
        raw = config_path.read_text(encoding="utf-8")
        clean_lines = [
            line for line in raw.splitlines()
            if not line.strip().startswith("#") and not line.strip().startswith("//")
        ]
        config_data = json.loads("\n".join(clean_lines))

        # Config file values as defaults, CLI takes precedence
        bot_endpoint = bot_endpoint or config_data.get("bot_endpoint", "")
        bot_api_key = bot_api_key or config_data.get("bot_api_key")
        bot_format = bot_format or config_data.get("bot_request_format", "openai")
        doc_text = doc_text or config_data.get("documentation", "")
        name = name if name != "CLI Simulation Run" else config_data.get("name", name)
        personas = personas if personas != 20 else config_data.get("num_personas", 20)
        max_turns = max_turns if max_turns != 15 else config_data.get("max_turns", 15)

        if not criteria_list:
            criteria_list = config_data.get("success_criteria", [])
        if not topics_list:
            topics_list = config_data.get("topics", [])

        console.print(f"  üìã Loaded config from [bold]{config_file}[/]")

    # Show what we're testing with
    if criteria_list:
        console.print(f"  ‚úÖ Success criteria: {len(criteria_list)} rules")
    if topics_list:
        console.print(f"  üéØ Test topics: {', '.join(topics_list)}")
    if doc_text:
        console.print(f"  üìñ Documentation: {len(doc_text)} chars loaded")

    asyncio.run(_run_simulation(
        bot_endpoint=bot_endpoint,
        bot_api_key=bot_api_key,
        bot_format=bot_format,
        documentation=doc_text,
        success_criteria=criteria_list,
        topics=topics_list,
        name=name,
        num_personas=personas,
        max_turns=max_turns,
        min_turns=min_turns,
        max_parallel=parallel,
        output_dir=output,
        export_formats=export_formats.split(","),
        preview_personas=preview_personas,
        pass_threshold=pass_threshold,
        warn_threshold=warn_threshold,
        auto_save_suite=auto_save_suite,
    ))


# ============================================================
# Partial Autonomous Mode Runner
# ============================================================

async def _run_partial_autonomous(
    bot_endpoint: str,
    bot_api_key: str | None,
    bot_format: str,
    doc_dir: str | None,
    doc_files: list[str] | None,
    auto_approve: bool,
    analysis_only: bool,
    output_dir: str,
    export_formats: list[str],
    simulation_name: str,
    num_personas: int | None = None,
    max_turns: int | None = None,
    min_turns: int = 1,
):
    """Run the partial autonomous pipeline."""
    from src.core.autonomous_orchestrator import AutonomousOrchestrator

    orch = AutonomousOrchestrator(
        bot_endpoint=bot_endpoint,
        doc_dir=doc_dir,
        doc_files=doc_files,
        bot_api_key=bot_api_key,
        bot_format=bot_format,
        auto_approve=auto_approve,
        output_dir=output_dir,
        export_formats=export_formats,
        simulation_name=simulation_name,
        num_personas=num_personas,
        max_turns=max_turns,
        min_turns=min_turns,
    )

    if analysis_only:
        console.print("\n[bold]Running analysis pipeline (no simulation)...[/]\n")
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task("Analyzing documents...", total=None)
            result = await orch.run_analysis_only()

        if result.aborted:
            console.print(f"\n[red]‚ùå Pipeline aborted: {result.abort_reason}[/]")
            return

        _print_analysis_results(result)
    else:
        console.print("\n[bold]Running full partial autonomous pipeline...[/]\n")
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task("Running pipeline...", total=None)
            result = await orch.run()

        if result.aborted:
            console.print(f"\n[red]‚ùå Pipeline aborted: {result.abort_reason}[/]")
            return

        _print_analysis_results(result)

        if result.simulation_report:
            _print_report(result.simulation_report, {})
            console.print(f"\n  üìÅ Results exported to: [bold]{output_dir}[/]")


def _print_analysis_results(result):
    """Display the analysis pipeline results."""
    from src.core.autonomous_orchestrator import AnalysisPipelineResult

    console.print("\n" + "=" * 60)
    console.print("[bold cyan]üìä Analysis Pipeline Results[/]")
    console.print("=" * 60)

    # Bot Context
    if result.bot_context:
        ctx = result.bot_context
        table = Table(title="Stage 1: Bot Context", show_header=False, box=None)
        table.add_column("Field", style="bold")
        table.add_column("Value")
        table.add_row("Bot Name", ctx.bot_name)
        table.add_row("Domain", ctx.domain)
        table.add_row("Purpose", ctx.purpose[:80] + "..." if len(ctx.purpose) > 80 else ctx.purpose)
        table.add_row("Capabilities", ", ".join(ctx.capabilities[:5]))
        table.add_row("Limitations", ", ".join(ctx.limitations[:3]))
        table.add_row("Confidence", f"[{'green' if ctx.confidence == 'high' else 'yellow'}]{ctx.confidence}[/]")
        console.print(table)

    # Success Criteria
    if result.criteria_set:
        console.print(f"\n[bold]Stage 2: Success Criteria[/] ({result.criteria_set.count} criteria)")
        for c in result.criteria_set.criteria[:5]:
            severity_color = "red" if c.importance == "high" else "yellow" if c.importance == "medium" else "white"
            console.print(f"  [{severity_color}]‚óè[/] [{c.category}] {c.criterion}")
        if result.criteria_set.count > 5:
            console.print(f"  ... and {result.criteria_set.count - 5} more")

    # Guardrails
    if result.guardrail_set:
        console.print(f"\n[bold]Stage 3: Guardrail Rules[/] ({result.guardrail_set.count} rules, "
                      f"{len(result.guardrail_set.critical_rules)} critical)")
        for r in result.guardrail_set.rules[:5]:
            sev_color = "red" if r.severity == "critical" else "yellow" if r.severity == "high" else "white"
            console.print(f"  [{sev_color}]‚óè[/] [{r.category}] {r.rule}")

    # Test Plan
    if result.test_plan:
        plan = result.test_plan
        console.print(f"\n[bold]Stage 4: Test Plan[/]")
        console.print(f"  Topics: {plan.topic_count} ({len(plan.high_risk_topics)} high-risk)")
        console.print(f"  Personas: {plan.persona_strategy.total_personas} "
                      f"({plan.persona_strategy.standard_pct}% standard, "
                      f"{plan.persona_strategy.edge_case_pct}% edge, "
                      f"{plan.persona_strategy.adversarial_pct}% adversarial)")
        console.print(f"  Turns: {plan.conversation_config.min_turns}-{plan.conversation_config.max_turns}")

        if plan.topics:
            topic_table = Table(title="Test Topics", show_lines=False)
            topic_table.add_column("Topic", style="bold")
            topic_table.add_column("Priority")
            topic_table.add_column("Risk")
            topic_table.add_column("Personas")
            for t in plan.topics:
                risk_color = "red" if t.risk_level == "high" else "yellow" if t.risk_level == "medium" else "green"
                topic_table.add_row(t.name, t.priority, f"[{risk_color}]{t.risk_level}[/]", str(t.estimated_personas))
            console.print(topic_table)

    # Gate summary
    if result.gate_manager:
        console.print(f"\n[bold]Approval Gates:[/] {result.gate_manager.summary}")

    console.print(f"\n  ‚è± Analysis time: {result.execution_time_seconds:.1f}s")
    console.print("=" * 60)


async def _run_simulation(
    bot_endpoint: str,
    bot_api_key: str | None,
    bot_format: str,
    documentation: str,
    success_criteria: list[str],
    topics: list[str],
    name: str,
    num_personas: int,
    max_turns: int,
    min_turns: int,
    max_parallel: int,
    output_dir: str,
    export_formats: list[str],
    preview_personas: bool = False,
    pass_threshold: float = 0.7,
    warn_threshold: float = 0.5,
    auto_save_suite: bool = False,
):
    """Async simulation runner."""
    from src.core.orchestrator import SimulationOrchestrator
    from src.models import BotConfig, SimulationConfig

    # Build documentation with topics if provided
    full_doc = documentation
    if topics:
        topics_text = "\n\nTest Focus Topics:\n" + "\n".join(f"- {t}" for t in topics)
        full_doc = (documentation + topics_text) if documentation else topics_text

    config = SimulationConfig(
        name=name,
        bot=BotConfig(
            api_endpoint=bot_endpoint,
            api_key=bot_api_key,
            request_format=bot_format,
        ),
        documentation=full_doc,
        success_criteria=success_criteria,
        num_personas=num_personas,
        max_turns_per_conversation=max_turns,
        min_turns_per_conversation=min_turns,
        max_parallel_conversations=max_parallel,
        pass_threshold=pass_threshold,
        warn_threshold=warn_threshold,
    )

    orchestrator = SimulationOrchestrator(config)

    # Persona preview mode
    if preview_personas:
        console.print("\n[bold]Generating personas for preview...[/]\n")
        personas = await orchestrator.generate_personas_only()
        _print_persona_preview(personas)

        if not click.confirm("\nProceed with simulation?", default=True):
            console.print("[yellow]Simulation cancelled.[/]")
            return

        # Run with approved personas
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task("Running simulation...", total=None)
            report = await orchestrator.run_simulation(personas=personas)
            progress.update(task, description="Exporting results...")
            exported = orchestrator.export_results(output_dir=output_dir, formats=export_formats)
    else:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task("Running simulation...", total=None)
            report = await orchestrator.run_simulation()
            progress.update(task, description="Exporting results...")
            exported = orchestrator.export_results(output_dir=output_dir, formats=export_formats)

    # Display results
    _print_report(report, exported)

    # Auto-save regression suite from failures
    if auto_save_suite:
        from src.regression.suite_manager import RegressionSuiteManager
        mgr = RegressionSuiteManager()
        suite = mgr.create_from_report(
            report=report,
            name=f"Regression: {name}",
            include_warnings=True,
        )
        if suite.total_cases > 0:
            suite_path = mgr.save_suite(suite, Path(output_dir) / "regression_suite.json")
            console.print(f"\n  üîÅ Regression suite saved: [bold]{suite_path}[/] ({suite.total_cases} test cases)")
        else:
            console.print("\n  ‚úÖ No failures found ‚Äî no regression suite needed")


# ============================================================
# simtest refine ‚Äî Iterative Persona Refinement
# ============================================================

@main.command()
@click.option("--report", "report_path", required=True, help="Path to summary.json from a previous run")
@click.option("--bot-endpoint", required=True, help="API endpoint of the bot to test")
@click.option("--bot-api-key", default=None, help="API key for the bot")
@click.option("--bot-format", default="openai", help="Request format: openai, anthropic, custom")
@click.option("--documentation", default="", help="Documentation text for grounding")
@click.option("--doc-file", default=None, help="Path to documentation file")
@click.option("--personas", default=10, help="Number of refined personas to generate")
@click.option("--max-turns", default=15, help="Max conversation turns")
@click.option("--output", default="./reports/refined", help="Output directory")
@click.option("--export-formats", default="jsonl,csv,summary,html", help="Export formats")
def refine(
    report_path: str,
    bot_endpoint: str,
    bot_api_key: str | None,
    bot_format: str,
    documentation: str,
    doc_file: str | None,
    personas: int,
    max_turns: int,
    output: str,
    export_formats: str,
):
    """Run iterative persona refinement based on a previous simulation's failures.

    Analyzes the previous report to find high-risk persona types, then generates
    new personas that drill deeper into those weak spots and re-runs the simulation.
    """
    from src.core.logging import setup_logging
    setup_logging()

    console.print(Panel.fit(
        "[bold magenta]AI SimTest[/] - Iterative Persona Refinement",
        subtitle="Drill deeper into failures",
    ))

    # Load previous report
    rp = Path(report_path)
    if not rp.exists():
        console.print(f"[red]Error: Report file not found: {report_path}[/]")
        sys.exit(1)

    # Load documentation
    doc_text = documentation
    if doc_file:
        dp = Path(doc_file)
        if dp.exists():
            doc_text = dp.read_text(encoding="utf-8")

    asyncio.run(_run_refinement(
        report_path=rp,
        bot_endpoint=bot_endpoint,
        bot_api_key=bot_api_key,
        bot_format=bot_format,
        documentation=doc_text,
        num_personas=personas,
        max_turns=max_turns,
        output_dir=output,
        export_formats=export_formats.split(","),
    ))


async def _run_refinement(
    report_path: Path,
    bot_endpoint: str,
    bot_api_key: str | None,
    bot_format: str,
    documentation: str,
    num_personas: int,
    max_turns: int,
    output_dir: str,
    export_formats: list[str],
):
    """Async refinement runner."""
    from src.core.orchestrator import SimulationOrchestrator
    from src.generators.persona_refiner import PersonaRefiner
    from src.models import BotConfig, SimulationConfig

    # Load the report files
    report_dir = report_path.parent
    jsonl_path = report_dir / "conversations.jsonl"

    console.print(f"  üìä Loaded previous report: {report_path}")

    # Reconstruct report from files
    report = _reconstruct_report_from_files(report_path, jsonl_path)

    # Analyze failures
    refiner = PersonaRefiner()
    analysis = refiner.analyze_failures(report)

    # Display analysis
    _print_refinement_analysis(analysis)

    if not analysis["high_risk_personas"]:
        console.print("\n[green]No high-risk personas found ‚Äî your bot is doing well![/]")
        return

    if not click.confirm("\nGenerate refined personas and run simulation?", default=True):
        console.print("[yellow]Refinement cancelled.[/]")
        return

    # Generate refined personas
    with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), console=console) as progress:
        task = progress.add_task("Generating refined personas...", total=None)
        refined_personas, _ = await refiner.generate_refined_personas(
            report=report,
            num_personas=num_personas,
            bot_description=f"Bot endpoint: {bot_endpoint}",
            documentation=documentation,
        )
        progress.update(task, description=f"Generated {len(refined_personas)} refined personas")

    if not refined_personas:
        console.print("[yellow]Could not generate refined personas.[/]")
        return

    # Preview refined personas
    _print_persona_preview(refined_personas)

    # Run simulation with refined personas
    config = SimulationConfig(
        name="Refinement Run",
        bot=BotConfig(
            api_endpoint=bot_endpoint,
            api_key=bot_api_key,
            request_format=bot_format,
        ),
        documentation=documentation,
        num_personas=len(refined_personas),
        max_turns_per_conversation=max_turns,
    )

    orchestrator = SimulationOrchestrator(config)

    with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), console=console) as progress:
        task = progress.add_task("Running refined simulation...", total=None)
        refined_report = await orchestrator.run_simulation(personas=refined_personas)
        progress.update(task, description="Exporting results...")
        exported = orchestrator.export_results(output_dir=output_dir, formats=export_formats)

    _print_report(refined_report, exported)
    console.print("\n[bold magenta]Refinement complete![/] Compare with original using: simtest compare")


# ============================================================
# simtest save-suite ‚Äî Save regression suite from report
# ============================================================

@main.command("save-suite")
@click.option("--report", "report_path", required=True, help="Path to summary.json from a simulation run")
@click.option("--conversations", "jsonl_path", default=None, help="Path to conversations.jsonl (auto-detected if in same dir)")
@click.option("--name", default="Regression Suite", help="Name for the suite")
@click.option("--output", default="./reports/regression_suite.json", help="Output file path")
@click.option("--include-passing", is_flag=True, default=False, help="Also include passing edge cases")
@click.option("--include-warnings/--no-include-warnings", default=True, help="Include WARNING cases (default: True)")
@click.option("--max-cases", default=100, help="Maximum test cases to save")
def save_suite(
    report_path: str,
    jsonl_path: str | None,
    name: str,
    output: str,
    include_passing: bool,
    include_warnings: bool,
    max_cases: int,
):
    """Save a regression test suite from simulation failures.

    Extracts failing conversations as frozen test cases that can be
    replayed later to check if issues got fixed or regressed.
    """
    console.print(Panel.fit(
        "[bold cyan]AI SimTest[/] - Save Regression Suite",
    ))

    rp = Path(report_path)
    if not rp.exists():
        console.print(f"[red]Error: Report not found: {report_path}[/]")
        sys.exit(1)

    # Auto-detect JSONL path
    jp = Path(jsonl_path) if jsonl_path else rp.parent / "conversations.jsonl"

    report = _reconstruct_report_from_files(rp, jp)

    from src.regression.suite_manager import RegressionSuiteManager
    mgr = RegressionSuiteManager()
    suite = mgr.create_from_report(
        report=report,
        name=name,
        include_passing=include_passing,
        include_warnings=include_warnings,
        max_cases=max_cases,
    )

    if suite.total_cases == 0:
        console.print("[green]No failing test cases found ‚Äî your bot passed everything![/]")
        return

    path = mgr.save_suite(suite, output)

    # Display summary
    table = Table(title=f"Regression Suite: {name}")
    table.add_column("Metric", style="bold")
    table.add_column("Value", justify="right")
    table.add_row("Total test cases", str(suite.total_cases))
    table.add_row("FAIL cases", str(sum(1 for tc in suite.test_cases if tc.original_label == "FAIL")))
    table.add_row("WARNING cases", str(sum(1 for tc in suite.test_cases if tc.original_label == "WARNING")))
    table.add_row("PASS (edge) cases", str(sum(1 for tc in suite.test_cases if tc.original_label == "PASS")))
    table.add_row("Saved to", str(path))
    console.print(table)

    # Show tags breakdown
    all_tags: dict[str, int] = {}
    for tc in suite.test_cases:
        for tag in tc.tags:
            all_tags[tag] = all_tags.get(tag, 0) + 1
    if all_tags:
        console.print("\n  Failure types in suite:")
        for tag, count in sorted(all_tags.items(), key=lambda x: x[1], reverse=True):
            console.print(f"    {tag}: {count} cases")

    console.print(f"\n  Replay later with: [bold]simtest replay --suite {path} --bot-endpoint <url>[/]")


# ============================================================
# simtest replay ‚Äî Replay regression suite
# ============================================================

@main.command()
@click.option("--suite", "suite_path", required=True, help="Path to regression_suite.json")
@click.option("--bot-endpoint", required=True, help="API endpoint of the bot to test")
@click.option("--bot-api-key", default=None, help="API key for the bot")
@click.option("--bot-format", default="openai", help="Request format: openai, anthropic, custom")
@click.option("--documentation", default="", help="Documentation text for grounding")
@click.option("--doc-file", default=None, help="Path to documentation file")
@click.option("--max-turns", default=15, help="Max conversation turns per test case")
@click.option("--output", default="./reports/replay", help="Output directory for replay results")
@click.option("--fail-on-regression", is_flag=True, default=False, help="Exit with code 1 if regressions found (for CI/CD)")
def replay(
    suite_path: str,
    bot_endpoint: str,
    bot_api_key: str | None,
    bot_format: str,
    documentation: str,
    doc_file: str | None,
    max_turns: int,
    output: str,
    fail_on_regression: bool,
):
    """Replay a regression suite against the bot and check for fixes/regressions.

    Sends the same user messages from saved test cases and judges the new
    responses, reporting what got fixed and what regressed.
    """
    from src.core.logging import setup_logging
    setup_logging()

    console.print(Panel.fit(
        "[bold yellow]AI SimTest[/] - Regression Replay",
        subtitle="Check fixes & regressions",
    ))

    sp = Path(suite_path)
    if not sp.exists():
        console.print(f"[red]Error: Suite not found: {suite_path}[/]")
        sys.exit(1)

    # Load documentation
    doc_text = documentation
    if doc_file:
        dp = Path(doc_file)
        if dp.exists():
            doc_text = dp.read_text(encoding="utf-8")

    exit_code = asyncio.run(_run_replay(
        suite_path=sp,
        bot_endpoint=bot_endpoint,
        bot_api_key=bot_api_key,
        bot_format=bot_format,
        documentation=doc_text,
        max_turns=max_turns,
        output_dir=output,
        fail_on_regression=fail_on_regression,
    ))

    if exit_code != 0:
        sys.exit(exit_code)


async def _run_replay(
    suite_path: Path,
    bot_endpoint: str,
    bot_api_key: str | None,
    bot_format: str,
    documentation: str,
    max_turns: int,
    output_dir: str,
    fail_on_regression: bool,
) -> int:
    """Async replay runner. Returns exit code."""
    from src.models import BotConfig
    from src.regression.suite_manager import RegressionSuiteManager

    mgr = RegressionSuiteManager()
    suite = mgr.load_suite(suite_path)

    console.print(f"  üìã Loaded suite: [bold]{suite.name}[/] ({suite.total_cases} test cases)")

    bot_config = BotConfig(
        api_endpoint=bot_endpoint,
        api_key=bot_api_key,
        request_format=bot_format,
    )

    with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), console=console) as progress:
        task = progress.add_task(f"Replaying {suite.total_cases} test cases...", total=None)
        summary = await mgr.replay_suite(
            suite=suite,
            bot_config=bot_config,
            documentation=documentation,
            max_turns=max_turns,
        )
        progress.update(task, description="Replay complete!")

    # Save results
    out_path = Path(output_dir)
    out_path.mkdir(parents=True, exist_ok=True)
    mgr.save_replay_summary(summary, out_path / "replay_summary.json")

    # Display results
    _print_replay_summary(summary)

    # CI/CD exit code
    if fail_on_regression and summary.regressed > 0:
        console.print(f"\n[bold red]CI FAILED: {summary.regressed} regression(s) detected![/]")
        return 1

    return 0


def _print_replay_summary(summary):
    """Print a formatted replay summary."""
    table = Table(title=f"Replay Results: {summary.suite_name}")
    table.add_column("Status", style="bold")
    table.add_column("Count", justify="right")
    table.add_column("Icon")

    table.add_row("Fixed", str(summary.fixed), "‚úÖ")
    table.add_row("Still Failing", str(summary.still_failing), "üî¥")
    table.add_row("Regressed", str(summary.regressed), "‚ùå")
    table.add_row("Still Passing", str(summary.still_passing), "üü¢")
    table.add_row("Improved", str(summary.improved), "üìà")
    table.add_row("Total", str(summary.total_cases), "")

    console.print(table)

    if summary.avg_score_change != 0:
        direction = "‚Üë" if summary.avg_score_change > 0 else "‚Üì"
        color = "green" if summary.avg_score_change > 0 else "red"
        console.print(f"\n  Average score change: [{color}]{direction} {abs(summary.avg_score_change):.3f}[/]")

    # Show details for regressions and fixes
    regressions = [r for r in summary.results if r.status == "regressed"]
    if regressions:
        console.print("\n[bold red]Regressions:[/]")
        for r in regressions:
            console.print(f"  ‚ùå {r.persona_name}: {r.original_label}‚Üí{r.new_label} (score: {r.original_score:.2f}‚Üí{r.new_score:.2f})")
            for f in r.new_failures[:2]:
                console.print(f"     ‚Üí {f[:80]}")

    fixes = [r for r in summary.results if r.status == "fixed"]
    if fixes:
        console.print("\n[bold green]Fixed:[/]")
        for r in fixes:
            console.print(f"  ‚úÖ {r.persona_name}: {r.original_label}‚Üí{r.new_label} (score: {r.original_score:.2f}‚Üí{r.new_score:.2f})")


# ============================================================
# simtest compare ‚Äî Compare two runs
# ============================================================

@main.command()
@click.argument("before_path")
@click.argument("after_path")
@click.option("--output", default=None, help="Save comparison report to JSON file")
def compare(before_path: str, after_path: str, output: str | None):
    """Compare two simulation reports side-by-side.

    Shows what improved, what regressed, new failures, and resolved failures.

    \b
    Usage: simtest compare reports/v1/summary.json reports/v2/summary.json
    """
    console.print(Panel.fit(
        "[bold green]AI SimTest[/] - Run Comparison",
        subtitle="Before vs After",
    ))

    bp = Path(before_path)
    ap = Path(after_path)

    if not bp.exists():
        console.print(f"[red]Error: Before report not found: {before_path}[/]")
        sys.exit(1)
    if not ap.exists():
        console.print(f"[red]Error: After report not found: {after_path}[/]")
        sys.exit(1)

    from src.comparison.engine import ComparisonEngine

    engine = ComparisonEngine()
    report = engine.compare_files(bp, ap)

    # Print formatted comparison
    console.print(engine.format_console(report))

    # Save if requested
    if output:
        engine.save_comparison(report, output)
        console.print(f"  üìÑ Comparison saved to: {output}")


# ============================================================
# Shared helpers
# ============================================================

def _reconstruct_report_from_files(summary_path: Path, jsonl_path: Path):
    """Reconstruct a SimulationReport from saved files (best effort)."""
    from src.models import (
        Conversation, FailurePattern, JudgedConversation, JudgedTurn,
        JudgmentLabel, JudgmentResult, Persona, PersonaType,
        ReportSummary, Severity, SimulationReport, Turn,
    )

    with open(summary_path) as f:
        summary_data = json.load(f)

    judged_conversations = []

    # Try to load full conversations from JSONL
    if jsonl_path.exists():
        with open(jsonl_path) as f:
            for line in f:
                try:
                    record = json.loads(line.strip())
                    turns = [
                        Turn(speaker=t["speaker"], message=t["message"], latency_ms=t.get("latency_ms"))
                        for t in record.get("turns", [])
                    ]
                    conv = Conversation(
                        id=record.get("conversation_id", "unknown"),
                        persona_id=record.get("persona_id", "unknown"),
                        turns=turns,
                    )
                    judged_turns = []
                    for jt_data in record.get("judgments", []):
                        judgments = [
                            JudgmentResult(
                                judge_name=jr["judge"],
                                passed=jr["passed"],
                                score=jr["score"],
                                message=jr.get("message", ""),
                            )
                            for jr in jt_data.get("judge_results", [])
                        ]
                        label = JudgmentLabel(jt_data.get("overall_label", "PASS"))
                        judged_turns.append(JudgedTurn(
                            turn=Turn(speaker="bot", message=""),
                            judgments=judgments,
                            overall_label=label,
                            overall_score=jt_data.get("overall_score", 0.0),
                            issues=jt_data.get("issues", []),
                        ))

                    persona = Persona(
                        name=record.get("persona_name", "Unknown"),
                        role="reconstructed",
                        goals=["reconstructed"],
                    )

                    jc = JudgedConversation(
                        conversation=conv,
                        persona=persona,
                        judged_turns=judged_turns,
                        overall_score=record.get("overall_score", 0.0),
                        failure_modes=record.get("failure_modes", []) if isinstance(record.get("failure_modes"), list) else [],
                    )
                    judged_conversations.append(jc)
                except Exception:
                    continue

    s = summary_data.get("summary", {})
    report = SimulationReport(
        summary=ReportSummary(
            simulation_id=s.get("simulation_id", "unknown"),
            simulation_name=s.get("simulation_name", "unknown"),
            total_personas=s.get("total_personas", 0),
            total_conversations=s.get("total_conversations", 0),
            total_turns=s.get("total_turns", 0),
            pass_rate=s.get("pass_rate", 0.0),
            average_score=s.get("average_score", 0.0),
            critical_failures=s.get("critical_failures", 0),
            warnings=s.get("warnings", 0),
            execution_time_seconds=s.get("execution_time_seconds", 0.0),
        ),
        failure_patterns=[
            FailurePattern(
                pattern_name=fp.get("pattern_name", ""),
                description=fp.get("description", ""),
                frequency=fp.get("frequency", 0),
                severity=Severity(fp.get("severity", "medium")),
            )
            for fp in summary_data.get("failure_patterns", [])
        ],
        score_by_judge=summary_data.get("score_by_judge", {}),
        score_by_persona_type=summary_data.get("score_by_persona_type", {}),
        recommendations=summary_data.get("recommendations", []),
        judged_conversations=judged_conversations,
    )
    return report


def _print_refinement_analysis(analysis: dict):
    """Print refinement analysis results."""
    console.print("\n[bold]Failure Analysis:[/]")

    by_type = analysis.get("failure_by_type", {})
    if by_type:
        console.print("\n  Failure rates by persona type:")
        for t, rate in sorted(by_type.items(), key=lambda x: x[1], reverse=True):
            bar = "‚ñà" * int(rate * 20) + "‚ñë" * (20 - int(rate * 20))
            color = "red" if rate > 0.5 else "yellow" if rate > 0.3 else "green"
            console.print(f"    {t:15s} [{color}]{bar}[/] {rate:.0%}")

    by_topic = analysis.get("failure_by_topic", {})
    if by_topic:
        console.print("\n  High-failure topics:")
        for topic, count in sorted(by_topic.items(), key=lambda x: x[1], reverse=True)[:5]:
            console.print(f"    üî¥ {topic}: {count} failures")

    strategy = analysis.get("refinement_strategy", [])
    if strategy:
        console.print("\n  [bold]Refinement strategy:[/]")
        for s in strategy:
            console.print(f"    ‚Üí {s}")

    high_risk = analysis.get("high_risk_personas", [])
    console.print(f"\n  Found [bold red]{len(high_risk)}[/] high-risk persona profiles (>50% failure rate)")


def _print_persona_preview(personas):
    """Print a persona preview table for user review."""
    table = Table(title=f"Generated Personas ({len(personas)})", show_header=True, show_lines=True)
    table.add_column("#", style="dim", width=3)
    table.add_column("Name", style="bold", max_width=30)
    table.add_column("Type", width=12)
    table.add_column("Tone", width=12)
    table.add_column("Tech Level", width=12)
    table.add_column("Goals", max_width=40)
    table.add_column("Topics", max_width=30)

    for i, p in enumerate(personas, 1):
        type_color = {"standard": "blue", "edge_case": "yellow", "adversarial": "red"}.get(
            p.persona_type.value, "white"
        )
        goals_str = ", ".join(p.goals[:2])
        topics_str = ", ".join(p.topics[:3]) if p.topics else "-"
        tactics = ""
        if p.adversarial_tactics:
            tactics = f"\n[dim]Tactics: {', '.join(p.adversarial_tactics[:2])}[/]"

        table.add_row(
            str(i),
            p.name,
            f"[{type_color}]{p.persona_type.value}[/]",
            p.tone,
            p.technical_level.value,
            goals_str + tactics,
            topics_str,
        )

    console.print(table)


def _print_report(report, exported: dict[str, str]):
    """Print a formatted report to the console."""

    s = report.summary

    table = Table(title="Simulation Results", show_header=True)
    table.add_column("Metric", style="bold")
    table.add_column("Value", justify="right")

    table.add_row("Total Personas", str(s.total_personas))
    table.add_row("Total Conversations", str(s.total_conversations))
    table.add_row("Total Turns", str(s.total_turns))
    table.add_row("Pass Rate", f"{s.pass_rate:.1%}")
    table.add_row("Average Score", f"{s.average_score:.2f}")
    table.add_row("Critical Failures", str(s.critical_failures))
    table.add_row("Warnings", str(s.warnings))
    table.add_row("Execution Time", f"{s.execution_time_seconds:.1f}s")

    console.print(table)

    if report.score_by_judge:
        console.print("\n[bold]Scores by Judge:[/]")
        for judge, score in report.score_by_judge.items():
            bar = "‚ñà" * int(score * 20) + "‚ñë" * (20 - int(score * 20))
            color = "green" if score >= 0.8 else "yellow" if score >= 0.6 else "red"
            console.print(f"  {judge:20s} [{color}]{bar}[/] {score:.1%}")

    if report.recommendations:
        console.print("\n[bold]Recommendations:[/]")
        for rec in report.recommendations:
            console.print(f"  {rec}")

    if exported:
        console.print("\n[bold]Exported Files:[/]")
        for fmt, path in exported.items():
            icon = "üìä" if fmt == "html" else "üìÑ"
            console.print(f"  {icon} {fmt}: {path}")
        if "html" in exported:
            console.print(f"\n  [bold green]‚Üí Open the HTML report in your browser: file://{Path(exported['html']).resolve()}[/]")

    console.print()


@main.command()
def serve():
    """Start the API server."""
    import uvicorn
    from src.core.config import settings

    console.print(Panel.fit(
        f"[bold blue]AI SimTest API Server[/]\nhttp://{settings.api_host}:{settings.api_port}",
    ))

    uvicorn.run(
        "src.api.app:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.app_env == "development",
    )


@main.command()
def version():
    """Show version info."""
    console.print("[bold]AI SimTest[/] v0.2.0")
    console.print("Open-source AI simulation testing platform")
    console.print("\nCommands:")
    console.print("  run          Run a simulation test")
    console.print("  refine       Iterative persona refinement")
    console.print("  save-suite   Save regression suite from failures")
    console.print("  replay       Replay regression suite")
    console.print("  compare      Compare two simulation reports")
    console.print("  serve        Start the API server")


if __name__ == "__main__":
    main()